# Discovery Processing Feature Debug Report

## Issue Summary
**Error**: UnicodeDecodeError when uploading PDF files to `/api/discovery/process` endpoint  
**Location**: `/app/src/api/discovery_endpoints.py`, line 96  
**Root Cause**: The endpoint was trying to parse binary PDF data as JSON UTF-8 text

## Fix Applied

### Problem
The endpoint was using `await request.json()` which expects UTF-8 encoded JSON data, but the frontend was sending raw binary PDF data.

### Solution
Modified the endpoint to handle multiple content types:
1. **JSON with base64-encoded files** (original expected format)
2. **Multipart/form-data** (standard file upload format)
3. **Raw binary data** (fallback for direct PDF uploads)

### Code Changes
Updated `/mnt/c/Users/jlemr/Test2/local-ai-package/Clerk/src/api/discovery_endpoints.py` lines 79-173:

```python
# Check content type to handle different request formats
content_type = request.headers.get("content-type", "")

if "application/json" in content_type:
    # Handle JSON request with base64-encoded files
    request_data = await request.json()
    # ... extract fields from JSON
else:
    # Handle multipart/form-data or raw binary upload
    try:
        form = await request.form()
        # ... process form data and files
    except Exception as form_error:
        # If form parsing fails, treat as raw binary upload
        body = await request.body()
        if body:
            # Convert to base64 for consistency with rest of pipeline
            discovery_files = [{
                "filename": "discovery_upload.pdf",
                "content": base64.b64encode(body).decode('utf-8'),
                "content_type": "application/pdf"
            }]
```

## Current Implementation Status

### Working Components ‚úÖ
1. **API Endpoints**: All discovery endpoints are implemented and handle uploads
2. **WebSocket Events**: Real-time updates are emitted during processing
3. **Fact Extraction**: Facts are extracted from documents using AI
4. **Document Processing**: Basic document processing with text extraction
5. **Error Handling**: Graceful handling of multiple upload formats

### Partially Working Components ‚ö†Ô∏è
1. **Document Splitting**: The `NormalizedDiscoveryProductionProcessor` exists and is initialized but:
   - It's properly integrated into the processing pipeline
   - The document splitting logic is called
   - However, need to verify if PDFs are actually being split into segments

### Not Yet Tested üîç
1. **Multi-document PDF Splitting**: Need to test with concatenated discovery PDFs
2. **Segment Processing**: Each segment should be processed as a separate document
3. **Qdrant Storage**: Verify documents and chunks are stored correctly
4. **Fact Deduplication**: Check if duplicate facts are properly filtered
5. **Box Integration**: Box folder upload functionality

## Testing Steps

### Quick Test (Without Rebuilding Container)
```bash
# Copy the fixed file directly to the running container
docker cp /mnt/c/Users/jlemr/Test2/local-ai-package/Clerk/src/api/discovery_endpoints.py localai-clerk-1:/app/src/api/discovery_endpoints.py

# Restart the Clerk service to apply changes
docker-compose -p localai -f docker-compose.yml -f docker-compose.clerk.yml restart clerk
```

### Test Scenarios
1. **Single PDF Upload**: Upload a single PDF file and verify processing
2. **Multi-document PDF**: Upload a concatenated discovery PDF to test splitting
3. **Multiple Files**: Upload multiple individual PDF files
4. **Large PDF**: Test with a 100+ page PDF for performance
5. **Error Cases**: Test with non-PDF files, corrupted PDFs

### Expected WebSocket Events
```javascript
// During successful processing:
discovery:started
discovery:document_found (multiple times for multi-doc PDFs)
discovery:chunking
discovery:embedding
discovery:fact_extracted (multiple times)
discovery:completed

// On errors:
discovery:error
```

## Next Steps

1. **Immediate Testing**: Copy fixed file to container and test basic upload
2. **Verify Document Splitting**: Test with multi-document PDF to ensure splitting works
3. **Check Qdrant Storage**: Query Qdrant to verify documents are stored
4. **Frontend Integration**: Ensure frontend sends correct content-type headers
5. **Performance Testing**: Test with large PDFs and multiple concurrent uploads

## Additional Notes

### Frontend Compatibility
The fix supports multiple upload methods, so the frontend can use any of:
- Direct binary upload with `application/octet-stream`
- Form data upload with `multipart/form-data`
- JSON upload with base64-encoded files

### Backward Compatibility
The fix maintains backward compatibility with the original JSON format while adding support for binary uploads.

### Security Considerations
- File size limits should be enforced
- File type validation (PDF only) should be verified
- Case isolation is maintained through `case_context`

## Monitoring
To monitor the fix in production:
```bash
# Watch Clerk logs for errors
docker logs -f localai-clerk-1

# Check for successful processing
docker logs localai-clerk-1 | grep "Processing discovery production"

# Look for any Unicode errors
docker logs localai-clerk-1 | grep -i "unicode"
```